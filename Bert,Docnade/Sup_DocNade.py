# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rHnIA9bvb-amn02mTMGDQdoCBqVVkkrG
"""

import torch
import time
import numpy as np

torch.cuda.is_available()

a = torch.randn(6000,1).cuda()

class DocNade():
  def __init__(self,vocab_size,hidden_layer_size,num_constructs,external_feature_size,external_feature,lamda,learning_rate, regulazr):
    #parameters
    self.vocab_size = vocab_size
    self.hidden_layer_size = hidden_layer_size
    self.num_constructs = num_constructs
    self.regulazr = regulazr
    self.external_feature_size = external_feature_size
    self.external_feature = external_feature
    self.lamda = lamda
    self.learning_rate = learning_rate
    self.alpha = torch.randn(1,vocab_size,dtype=torch.float64).cuda()
    self.alpha_grad = torch.zeros(1,vocab_size,dtype=torch.float64).cuda()
    self.u = torch.randn(vocab_size,hidden_layer_size,dtype=torch.float64).cuda()/(vocab_size*hidden_layer_size)
    self.u[0,:] = 0.000000000000000000000
    self.u_grad = torch.zeros(vocab_size,hidden_layer_size,dtype=torch.float64).cuda()
    self.b = torch.randn(vocab_size,1,dtype=torch.float64).cuda()/(vocab_size*1)
    self.b[0,0] = 0.000000000000000000000
    self.b_grad = torch.zeros(vocab_size,1,dtype=torch.float64).cuda()
    self.w = torch.randn(hidden_layer_size,vocab_size,dtype=torch.float64).cuda()/(hidden_layer_size*vocab_size*200)
    self.w_grad = torch.zeros(hidden_layer_size,vocab_size,dtype=torch.float64).cuda()
    self.c = torch.randn(hidden_layer_size,1,dtype=torch.float64).cuda()/(hidden_layer_size*1)
    self.c_grad = torch.zeros(hidden_layer_size,1,dtype=torch.float64).cuda()
    self.s = torch.randn(num_constructs,hidden_layer_size+external_feature_size,dtype=torch.float64).cuda()/(num_constructs*hidden_layer_size+external_feature_size)
    self.s_grad = torch.zeros(num_constructs,hidden_layer_size+external_feature_size,dtype=torch.float64).cuda()
    self.d = torch.randn(num_constructs,1,dtype=torch.float64).cuda()/(num_constructs*1)
    self.d_grad = torch.zeros(num_constructs,1,dtype=torch.float64).cuda()

    #hidden layer
    self.pre_activation = torch.zeros(hidden_layer_size,1,dtype=torch.float64).cuda()
    self.post_activation = torch.zeros(hidden_layer_size,1,dtype=torch.float64).cuda()

    #output layer
    self.pre_softmax = torch.zeros(vocab_size,1,dtype=torch.float64).cuda()
    self.prob = torch.zeros(vocab_size,1,dtype=torch.float64).cuda()

    #supervised input layer
    self.sup_pre_activation = torch.zeros(hidden_layer_size+external_feature_size,1,dtype=torch.float64).cuda()
    self.sup_post_activation = torch.zeros(hidden_layer_size+external_feature_size,1,dtype=torch.float64).cuda()

    #supervised output layer
    self.pre_softmax1 = torch.zeros(num_constructs,1,dtype=torch.float64).cuda()
    self.prob1 = torch.zeros(num_constructs,1,dtype=torch.float64).cuda()
    self.m = torch.nn.Softmax(dim=0)

  def softmax(self,z):
    output = torch.exp(z)
    normal = torch.sum(output)
    output = output/normal
    return output
  
  def forward(self,x,y,doc,require_grad,require_map):
    temp = torch.zeros(self.vocab_size,1).cuda()
    h_grad = torch.zeros(self.hidden_layer_size,1).cuda()
    output = torch.zeros(len(y),2).cuda()
    for i in range(len(doc)):
      #print(i)
      if require_grad:
        for j in range(len(x[i])):
          self.pre_activation = self.c
          for k in range(len(x[i][j])-1):
            self.pre_activation  = self.pre_activation + self.w[:,x[i][j][k]].reshape([self.hidden_layer_size,1])
            self.post_activation = torch.tanh(self.pre_activation)
            self.pre_softmax = torch.mm(self.u,self.post_activation) + self.b
            
            self.prob = self.m(self.pre_softmax)
            # print(self.prob)
            if require_grad:
              temp = -self.prob
              temp[x[i][j][k]] = temp[x[i][j][k]] + 1
              self.u_grad += self.lamda*torch.mm(temp,self.post_activation.reshape([1,self.hidden_layer_size]))/(len(x[i][j])*len(x[i])*len(x))
              
              self.b_grad += self.lamda*temp/(len(x[i][j])*len(x[i])*len(x))
              
              h_grad = self.u[x[i][j][k],:].reshape([self.hidden_layer_size,1]) - torch.mm(self.u.reshape([self.hidden_layer_size,self.vocab_size]),self.prob)
              h_grad = h_grad/(len(x[i][j])*len(x[i])*len(x))
              h_grad = h_grad*(1-self.post_activation*self.post_activation)
              
              for l in range(1,k+1):
                
                self.w_grad[:,x[i][j][l]] += self.lamda*h_grad.reshape(self.w_grad[:,x[i][j][l]].shape)
              self.c_grad += self.lamda*h_grad
      weights = torch.tensor([self.alpha[0,m] for m in doc[i]]).cuda()
      weights = weights.reshape([-1,1])
      temp = self.w[:,[m for m in doc[i]]]
      temp = (torch.mm(temp,weights) + self.c.reshape([-1,1]))
      #print(i)
      #print(temp)
      sup_h = temp
      self.sup_post_activation = torch.tanh(sup_h)
      #print(self.sup_post_activation)
      self.pre_softmax1 = torch.mm(self.s,self.sup_post_activation) + self.d
      #print(self.pre_softmax1)
      self.prob1 = self.softmax(self.pre_softmax1)
      #print(self.prob1)
      temp = -self.prob1
      temp[y[i]] = temp[y[i]]+1
      if require_map:
        #print("hi")
        output[i,1],output[i,0] = torch.max(self.prob1,0)
      else:
        #print(y[i])
        output[i,1] = self.prob1[y[i],0]
        output[i,0] = float(y[i])

      if require_grad:
        self.s_grad += torch.mm(temp,self.sup_post_activation.reshape([1,-1]))/len(x)
        self.d_grad += temp/len(x)
        #print(self.s)
        sup_h_grad = self.s[y[i],:].reshape([-1,1]) - torch.mm(self.s.reshape([-1,self.num_constructs]),self.prob1)
        #print(self.prob1)
        #print(torch.mm(self.s.reshape([-1,self.num_constructs]),self.prob1))
        #print(sup_h_grad)
        sup_h_grad = sup_h_grad/len(x)
        sup_h_grad = sup_h_grad*(1-self.sup_post_activation*self.sup_post_activation)
        #print(sup_h_grad)
        l1 = 0
        for l in doc[i]:
          self.w_grad[:,l] += (sup_h_grad*weights[l1]).reshape(self.w_grad[:,l].shape)
          
          self.alpha_grad[0,l] += torch.dot(sup_h_grad.reshape([1,-1]).squeeze(),self.w[:,l]).reshape([-1,1]).squeeze() 
          l1 = l1+1
        self.c_grad += sup_h_grad
    self.b_grad[0,0] = self.b_grad[0,0]/100
    self.u_grad[0,:] = self.u_grad[0,:]/100
    

    
    return output
  def backward(self):
    print(self.alpha_grad)
    print(self.u_grad)
    print(self.b_grad)
    print(self.w_grad)
    print(self.c_grad)
    print(self.s_grad)
    print(self.d_grad)
    print("hi")
    print(self.alpha)
    print(self.u)
    print(self.b)
    print(self.w)
    print(self.c)
    print(self.s)
    print(self.d)
    self.u += self.learning_rate*(self.u_grad - self.regulazr*self.u) 
    self.b += self.learning_rate*(self.b_grad/5 - self.regulazr*self.b/5)
    self.w += self.learning_rate*(self.w_grad/2 - self.regulazr*self.w/2) 
    
    self.c += self.learning_rate*(self.c_grad - self.regulazr*self.c)
    
    self.s += self.learning_rate*(self.s_grad - self.regulazr*self.s)
    self.d += self.learning_rate*(self.d_grad - self.regulazr*self.d)
    self.alpha += self.learning_rate*(self.alpha_grad - self.regulazr*self.alpha)
    self.u_grad = torch.zeros(self.u.shape,dtype=torch.float64).cuda()
    self.b_grad = torch.zeros(self.b.shape,dtype = torch.float64).cuda()
    self.w_grad = torch.zeros(self.w.shape,dtype = torch.float64).cuda()
    self.c_grad = torch.zeros(self.c.shape,dtype = torch.float64).cuda()
    self.s_grad = torch.zeros(self.s.shape,dtype = torch.float64).cuda()
    self.d_grad = torch.zeros(self.d.shape,dtype = torch.float64).cuda()
    self.alpha_grad = torch.zeros(self.alpha.shape,dtype = torch.float64).cuda()
    return 1
  def train(self,doc,x,y):
    #print(self.c)
    for epoch in range(150):
      print(epoch)
      output = self.forward(x,y,doc,True,True)
      self.backward()
      #print(output[:,0])
      y1 = torch.FloatTensor(y).cuda()
      print("Epoch: ",epoch,"\nAccuracy = ",torch.sum(output[:,0].eq(y1),dtype = float)/len(y)*100.0)

def ap(y_pred,y_belong):
  y_pred = np.array(y_pred)
  y_belong = np.array(y_belong)
  y_pred = y_pred.reshape(-1,1)
  y_belong = y_belong.reshape(-1,1)
  arr = np.concatenate((y_pred,y_belong),axis=1)
  
  arr = sorted(arr,key = lambda x:x[0],reverse=True)
  
  ans = 0
  rel = 0
  num = 0
  for i in range(len(y_pred)):
    if arr[i][1] == 1:
      rel = rel+1
      ans += rel/(i+1)
      num = num+1
  return ans/num

def map(y_pred,y,y_belong):
  j = 0
  j_prev = 0
  ans = 0
  for i in range(8):
    while j<len(y) and y[j] == i:
      j = j+1
    ans = ans + ap(y_pred[j_prev:j],y_belong[j_prev:j])
    j_prev = j
  return ans/8

a = DocNade(7142,50,8,0,external,0.5,0.5,0.00005)

out = a.train(dat,x,y)

import pickle

pickle_in = open("Train_Data.pkl","rb")
data = pickle.load(pickle_in)

dat = []
y = []
for i in range(266):
    dat.append(data.get(i)[1])
    y.append(data.get(i)[0])

pickle_in = open("x.pkl","rb")
x = pickle.load(pickle_in)

alpha = torch.ones(8,5280,dtype=torch.float64).cuda()

external =1

torch.save(a.u,"u50.pt")
torch.save(a.b,"b50.pt")
torch.save(a.w,"w50.pt")
torch.save(a.c,"c50.pt")
torch.save(a.s,"s50.pt")
torch.save(a.d,"d50.pt")
torch.save(a.alpha,"alpha50.pt")

a.u = torch.load("u50.pt")
a.b = torch.load("b50.pt")
a.c = torch.load("c50.pt")
a.w = torch.load("w50.pt")
a.s = torch.load("s50.pt")
a.d = torch.load("d50.pt")
a.alpha = torch.load("alpha50.pt")

pickle_in = open("Test_Data.pkl","rb")
test = pickle.load(pickle_in)

test_data = []
test_y = []
test_belong = []
for i in range(999):
    test_data.append(test.get(i)[2])
    test_y.append(test.get(i)[0])
    test_belong.append(test.get(i)[1])

ans = a.forward(x,test_y,test_data,False,False)

y1 = []
for i in range(len(test_y)):
  if test_belong[i]==1:
    y1.append(test_y[i])

y1 = torch.FloatTensor(y1).cuda()
indices = test_belong == np.ones(len(test_belong))

sum(ans[indices,0].eq(y1))/(786.0)*100

sum(ans[indices,:][~ans[indices,0].eq(y1),1])/174.0



test_y

ans.shape

map(ans[:,1].cpu(),test_y,test_belong)

ans[:,1]

def ap(y_pred,y_belong):
  y_pred = np.array(y_pred)
  y_belong = np.array(y_belong)
  y_pred = y_pred.reshape(-1,1)
  y_belong = y_belong.reshape(-1,1)
  arr = np.concatenate((y_pred,y_belong),axis=1)
  
  arr = sorted(arr,key = lambda x:x[0],reverse=True)
  
  ans = 0
  rel = 0
  num = 0
  for i in range(len(y_pred)):
    if arr[i][1] == 1:
      rel = rel+1
      ans += rel/(i+1)
      num = num+1
  return ans/num

def map(y_pred,y,y_belong):
  j = 0
  j_prev = 0
  ans = 0
  for i in range(8):
    while j<len(y) and y[j] == i:
      j = j+1
    ans = ans + ap(y_pred[j_prev:j],y_belong[j_prev:j])
    j_prev = j
  return ans/8

