# -*- coding: utf-8 -*-
"""Bert_Task_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vdW0ubVnCsn-vu-XOMTJj06PdY1uanKx
"""

!pip install transformers
import pandas as pd
import pickle
import nltk
import gensim
from gensim.parsing.preprocessing import remove_stopwords
nltk.download('punkt')
import json
import torch
import numpy as np
import pandas as pd
from transformers import BertModel, BertTokenizer, BertForSequenceClassification

labels = {0:"Acute_Threat_Fear",1:"Loss",2:"Arousal",3:"Circadian_Rhythms",4:"Frustrative_Nonreward",5:"Potential_Threat_Anxiety",6:"Sleep_Wakefulness",7:"Sustained_Threat"}
# def0 = "Activation of the brainâ€™s defensive motivational system to protect the organism from perceived danger. Normal fear involves adaptive responses to conditioned or unconditioned threat stimuli (exteroceptive or interoceptive). internal representations and cognitive processing."
# def5 = "Activation of a brain system in which harm may potentially occur but is distant, ambiguous, or low in probability, characterized by responses such as enhanced vigilance. responses to low imminence threats different than the high imminence threat behaviors that characterize fear"
# def7 = "An aversive emotional state caused by prolonged (i.e., weeks to months) exposure to internal and/or external condition(s), state(s), or stimuli that are adaptive to escape or avoid. The exposure may be actual or anticipated. the changes in affect, cognition, physiology, and behavior caused by sustained threat persist in the absence of the threat, and can be differentiated from those changes evoked by acute threat."
# def1 = "A state of deprivation of a motivationally significant con-specific, object, or situation. Loss may be social or non-social and may include permanent or sustained loss of shelter, behavioral control, status, loved ones, or relationships. The response to loss may be episodic (e.g., grief) or sustained."
# def4 = "Reactions elicited in response to withdrawal/prevention of reward, i.e., by the inability to obtain positive rewards following repeated or sustained efforts"
# def2 = "Arousal is a continuum of sensitivity of the organism to stimuli, both external and internal. Facilitates interaction with the environment in a context-specific manner (e.g., under conditions of threat, some stimuli must be ignored while sensitivity to and responses to others is enhanced, as exemplified in the startle reflex). Can be evoked by either external/environmental stimuli or internal stimuli (e.g., emotions and cognition), Can be modulated by the physical characteristics and motivational significance of stimuli, Varies along a continuum that can be quantified in any behavioral state, including wakefulness and low-arousal states including sleep, anesthesia, and coma, Is distinct from motivation and valence but can covary with intensity of motivation and valence, May be associated with increased or decreased locomotor activity, and Can be regulated by homeostatic drives (e.g., hunger, sleep, thirst, sex)."
# def3 = "Circadian Rhythms are endogenous self-sustaining oscillations that organize the timing of biological systems to optimize physiology and behavior, and health, Are synchronized by recurring environmental cues, Anticipate the external environment, Allow effective response to challenges and opportunities in the physical and social environment, Modulate homeostasis within the brain and other (central/peripheral) systems, tissues and organs. Are evident across levels of organization including molecules, cells, circuits, systems, organisms, and social systems."
# def6 = "Sleep and wakefulness are endogenous, recurring, behavioral states that reflect coordinated changes in the dynamic functional organization of the brain and that optimize physiology, behavior, and health, Homeostatic and circadian processes regulate the propensity for wakefulness and sleep Is reversible, typically characterized by postural recumbence, behavioral quiescence, and reduced responsiveness, Has a complex architecture with predictable cycling of NREM/REM states or their developmental equivalents, NREM and REM sleep have distinct neural substrates (circuitry, transmitters, modulators) and EEG oscillatory properties. Intensity and duration is affected by homeostatic regulation, Is affected by experiences during wakefulness, Is evident at cellular, circuit, and system levels, Has restorative and transformative effects that optimize neurobehavioral functions during wakefulness."
# def0 = remove_stopwords(def0)
# def1 = remove_stopwords(def1)
# def2 = remove_stopwords(def2)
# def3 = remove_stopwords(def3)
# def4 = remove_stopwords(def4)
# def5 = remove_stopwords(def5)
# def6 = remove_stopwords(def6)
# def7 = remove_stopwords(def7)

# def0 = nltk.tokenize.sent_tokenize(def0)
# def1 = nltk.tokenize.sent_tokenize(def1)
# def2 = nltk.tokenize.sent_tokenize(def2)
# def3 = nltk.tokenize.sent_tokenize(def3)
# def4 = nltk.tokenize.sent_tokenize(def4)
# def5 = nltk.tokenize.sent_tokenize(def5)
# def6 = nltk.tokenize.sent_tokenize(def6)
# def7 = nltk.tokenize.sent_tokenize(def7)
# defn = def0+def1+def2+def3+def4+def5+def6+def7
# len(defn)

def preprocess(defn,maxlen = 300):
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  token_id = []
  attn = []
  for x in defn:
    tokens = tokenizer.tokenize(x)
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    if len(tokens) < maxlen:
        tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))]
    else:
        tokens = tokens[:maxlen-1] + ['[SEP]']
    
    # Convert the sequence to ids with BERT Vocabulary
    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)
    token_id.append(tokens_ids)
    # Obtaining the attention mask
    # print(tokens_ids)
    attn_mask = (torch.tensor(tokens_ids) != 0).long()
    # print(attn_mask)
    attn.append(attn_mask.tolist())
  
  token_ids_tensor = torch.tensor(token_id)
  attn_tensor = torch.tensor(attn)
  # print(token_ids_tensor,attn)

  return token_ids_tensor, attn_tensor

# defn_token, defn_attn = preprocess(defn)
# print(len(defn_token),len(defn_attn))
# defn_token, defn_attn = defn_token.to('cuda'), defn_attn.to('cuda')

data1 = pd.read_excel(labels[0]+".xlsx")
data2 = pd.read_excel(labels[1]+".xlsx")
data3 = pd.read_excel(labels[2]+".xlsx")
data4 = pd.read_excel(labels[3]+".xlsx")
data5 = pd.read_excel(labels[4]+".xlsx")
data6 = pd.read_excel(labels[5]+".xlsx")
data7 = pd.read_excel(labels[6]+".xlsx")
data8 = pd.read_excel(labels[7]+".xlsx")

data1["Label"] = 1
data2["Label"] = 0
data3["Label"] = 0
data4["Label"] = 0
data5["Label"] = 0
data6["Label"] = 0
data7["Label"] = 0
data8["Label"] = 0

data = pd.concat([data1,data2,data3,data4,data5,data6,data7,data8],ignore_index=True)

for i in range(data.shape[0]):
    for j in range(1,3):
        
        data.iloc[i,j] = data.iloc[i,j].lower()

data

for i in range(data.shape[0]):
  data.iloc[i,2] = data.iloc[i,1] + " " + data.iloc[i,2]

data = data.sample(frac=1).reset_index(drop=True)

data

bert_model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

from torch.utils.data import Dataset, DataLoader

class LoadDataset(Dataset):

    def __init__(self, df, maxlen):

        # Store the contents of the file in a pandas dataframe
        self.df = df

        # Initialize the BERT tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        # Define the Maxlength for padding/truncating
        self.maxlen = maxlen

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):

        # Selecting the sentence and label at the specified index in the data frame
        sentence = self.df.iloc[index, 2]
        label = self.df.iloc[index, 3]
        #weight = self.df.iloc[index,4]
        # Tokenize the sentence
        tokens = self.tokenizer.tokenize(sentence)

        # Inserting the CLS and SEP token at the beginning and end of the sentence
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        # Padding/truncating the sentences to the maximum length
        if len(tokens) < self.maxlen:
            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]
        else:
            tokens = tokens[:self.maxlen-1] + ['[SEP]']
        
        # Convert the sequence to ids with BERT Vocabulary
        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        
        # Converting the list to a pytorch tensor
        tokens_ids_tensor = torch.tensor(tokens_ids)

        # Obtaining the attention mask
        attn_mask = (tokens_ids_tensor != 0).long()

        return tokens_ids_tensor, attn_mask, label

# Creating instances of training and validation set
train_set = LoadDataset(data, maxlen = 300)
#val_set = LoadDataset(filename = 'data/validation.csv', maxlen = 64)

train_loader = DataLoader(train_set, batch_size = 12)

from torch import nn

class Bert_Classifier(nn.Module):

    def __init__(self, freeze_bert = True):
        super(Bert_Classifier, self).__init__()

        # Instantiating the BERT model object 
        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')
        
        # Defining layers like dropout and linear
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(768, 2)

    def forward(self, seq, attn_masks):
        '''
        Inputs:
            -seq : Tensor of shape [B, T] containing token ids of sequences
            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens
        '''

        # Getting contextualized representations from BERT Layer
        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)
        # reps, _ = self.bert_layer(defn_token,defn_attn)
        # Obtaining the representation of [CLS] head
        cls_rep = cont_reps[:, 0]
        # defn_reps = reps[:,0]
        # print('CLS shape: ',cls_rep.shape)
        # presoft = torch.mm(cls_rep,defn_reps.reshape([-1,769]))

        # Feeding cls_rep to the classifier layer
        soft = self.classifier(cls_rep)
        # print('Logits shape: ',logits.shape)

        return soft

model = Bert_Classifier()

from torch.optim import Adam
from torch.nn import CrossEntropyLoss

# criterion = CrossEntropyLoss(torch.FloatTensor([data.shape[0]/(data1.shape[0]*10),data.shape[0]/(data2.shape[0]*10),data.shape[0]/(data3.shape[0]*10),data.shape[0]/(data4.shape[0]*10),data.shape[0]/(data5.shape[0]*10),data.shape[0]/(data6.shape[0]*10),data.shape[0]/(data7.shape[0]*10),data.shape[0]/(data8.shape[0]*10)]).cuda())
criterion = CrossEntropyLoss(torch.FloatTensor([data.shape[0]/data1.shape[0]*2,data.shape[0]/(data.shape[0]-data1.shape[0])]).cuda())
optimizer = Adam(model.parameters(), lr = 0.00005, weight_decay = 0.00001)

device = 'cuda'

# Defining a function for calculating accuracy
def softmax(z):
  out = torch.exp(z)
  norm = torch.sum(out)
  return out/norm
def accuracy(soft, labels):
    probs = softmax(soft.unsqueeze(-1))
    _ , preds = torch.max(probs,1) 
    print(preds,labels)
    acc = (preds.squeeze() == labels).float().mean()
    return acc

from time import time

def train(net, criterion, optimizer, train_loader, device, epochs=10, print_every=10):
    
    # Move model to device
    net.to(device)
    # Setting model to training mode
    net.train()

    print('========== ========== STARTING TRAINING ========== ==========')

    for epoch in range(epochs):

        print('\n\n========== EPOCH {} =========='.format(epoch))        
        t1 = time()
        # defn_token, defn_attn = defn_token.to(device), defn_attn.to(device)
        for i, (seq, attn_masks, labels) in enumerate(train_loader):

            # Clear gradients
            optimizer.zero_grad()  

            # Moving tensors to device
            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)
            
            # Obtaining the logits from the model
            soft = net(seq,attn_masks)
            # Calculating the loss
            loss = criterion(soft.squeeze(-1), labels.long())

            # Backpropagating the gradients
            loss.backward()

            # Clipping gradients to tackle exploding gradients
            nn.utils.clip_grad_norm_(net.parameters(), 1)

            # Optimization step
            optimizer.step()
            
            prob = softmax(soft)
            acc = accuracy(soft,labels)
            print("Accuracy = ",acc)
            seq, attn_masks, labels = seq.to('cpu'), attn_masks.to('cpu'), labels.to('cpu')
            if (i + 1) % print_every == 0:
                
                print("Iteration {} ==== Loss: {}".format(i+1, loss.item()))

        t2 = time()

        print('Time Taken for Epoch: {}'.format(t2-t1))

# starting training
train(model, criterion, optimizer, train_loader, device, epochs=5, print_every=1)

torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict()
}, 'model.pth')

# model.to('cuda')

# inference_file = torch.load('model.pth')
# predictor = Bert_Classifier()
# predictor.load_state_dict(inference_file['model_state_dict'])

# predictor.to("cuda")

Test_data1 = pd.read_excel(labels[0]+".xlsx")
Test_data2 = pd.read_excel(labels[1]+".xlsx")
Test_data3 = pd.read_excel(labels[2]+".xlsx")
Test_data4 = pd.read_excel(labels[3]+".xlsx")
Test_data5 = pd.read_excel(labels[4]+".xlsx")
Test_data6 = pd.read_excel(labels[5]+".xlsx")
Test_data7 = pd.read_excel(labels[6]+".xlsx")
Test_data8 = pd.read_excel(labels[7]+".xlsx")

Test_data1["Label"] = 1
Test_data2["Label"] = 0
Test_data3["Label"] = 0
Test_data4["Label"] = 0
Test_data5["Label"] = 0
Test_data6["Label"] = 0
Test_data7["Label"] = 0
Test_data8["Label"] = 0

Test_data = pd.concat([Test_data1,Test_data2,Test_data3,Test_data4,Test_data5,Test_data6,Test_data7,Test_data8],ignore_index=True)

for i in range(Test_data.shape[0]):
    for j in range(1,3):
        
        Test_data.iloc[i,j] = Test_data.iloc[i,j].lower()

for i in range(Test_data.shape[0]):
  Test_data.iloc[i,2] = Test_data.iloc[i,1] + " " + Test_data.iloc[i,2]

Test_data

class LoadTestDataset(Dataset):

    def __init__(self, df, maxlen):

        # Store the contents of the file in a pandas dataframe
        self.df = df

        # Initialize the BERT tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        # Define the Maxlength for padding/truncating
        self.maxlen = maxlen

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):

        # Selecting the sentence and label at the specified index in the data frame
        sentence = self.df.iloc[index, 2]
        label = self.df.iloc[index, 5]
        belong = self.df.iloc[index,3]
        # Tokenize the sentence
        tokens = self.tokenizer.tokenize(sentence)

        # Inserting the CLS and SEP token at the beginning and end of the sentence
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        # Padding/truncating the sentences to the maximum length
        if len(tokens) < self.maxlen:
            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]
        else:
            tokens = tokens[:self.maxlen-1] + ['[SEP]']
        
        # Convert the sequence to ids with BERT Vocabulary
        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        
        # Converting the list to a pytorch tensor
        tokens_ids_tensor = torch.tensor(tokens_ids)

        # Obtaining the attention mask
        attn_mask = (tokens_ids_tensor != 0).long()

        return tokens_ids_tensor, attn_mask, label, belong

test_set = LoadTestDataset(Test_data1, maxlen = 300)
test_loader = DataLoader(test_set, batch_size = 1)

def predict(net, test_set):
    device = 'cuda'
    # Setting model to evaluation mode
    net.eval()
    y_prob = []
    y_real = []
    y_belong = []
    # defn_token, defn_attn = defn_token.to(device), defn_attn.to(device)
    for i, (seq,attn_masks,y,belong) in enumerate(test_loader):
      seq, attn_masks = seq.to(device), attn_masks.to(device)
      y, belong = y.to(device), belong.to(device)
      soft = net(seq,attn_masks)
      # soft = torch.tensor(np.ones(8))
      output = softmax(soft).flatten()
      print(output[y].item())
      print(y.item())
      print(belong.item())
      seq, attn_masks = seq.to('cpu'), attn_masks.to('cpu')
      y, belong = y.to('cpu'), belong.to('cpu')
      y_prob.append(output[y].item())
      y_real.append(y.item())
      y_belong.append(belong.item())
    
    return y_prob, y_real, y_belong

y_prob, y_real, y_belong = predict(model,test_set)

def ap(y_pred,y_belong):
  y_pred = np.array(y_pred)
  y_belong = np.array(y_belong)
  y_pred = y_pred.reshape(-1,1)
  y_belong = y_belong.reshape(-1,1)
  arr = np.concatenate((y_pred,y_belong),axis=1)
  
  arr = sorted(arr,key = lambda x:x[0],reverse=True)
  
  ans = 0
  rel = 0
  num = 0
  for i in range(len(y_pred)):
    if arr[i][1] == 1:
      rel = rel+1
      ans += rel/(i+1)
      num = num+1
  print(ans/num)
  return ans/num

def map(y_pred,y,y_belong):
  j = 0
  j_prev = 0
  ans = 0
  for i in range(8):
    while j<len(y) and y[j] == i:
      j = j+1
    ans = ans + ap(y_pred[j_prev:j],y_belong[j_prev:j])
    j_prev = j
  return ans/8

ap(y_prob,y_belong)

[data.shape[0]/(data1.shape[0]*10),data.shape[0]/(data2.shape[0]*10),data.shape[0]/(data3.shape[0]*10),data.shape[0]/(data4.shape[0]*10),data.shape[0]/(data5.shape[0]*10),data.shape[0]/(data6.shape[0]*10),data.shape[0]/(data7.shape[0]*10),data.shape[0]/(data8.shape[0]*10)]

predictor.to('cpu')

!torch.cuda.empty_cache()



a,_ = bert_layer(defn_token,defn_attn)

a[:,0].reshape([-1,21])

defn_attn

torch.cuda.clear_cache()

!nvidia-smi

!kill 00000000:00:04.0

def my_map(test_set, y_test, y_pred, ind):
    ans = pd.DataFrame()
    ans['id'] = test_set['pmid']
    ans['label'] = y_test
    ans.sort_values(by=['label'], ascending = False, inplace = True)

    sol = pd.DataFrame()
    sol['id'] = test_set['pmid']
    # sol['prob'] = y_pred[:, ind]     
    sol['prob'] = y_pred     
    """
    for i in range(len(sol)):
        y_pred[i] = list(y_pred[i]).remove(max(y_pred[i]))
        sol['prob']-= max(y_pred[i])
    """
    sol.sort_values(by=['prob'], ascending = False, inplace = True)
    
    n = sum(ans['label'])
    l = len(ans)
    #print("len = ", l)
    #print("sum = ", n)
    
    tot_val = 0
    tot_got = 0

    for i in range(len(sol)):
        cur_id = sol['id'][i]
        cur_df = ans[ans['id'] == cur_id]
        if(sum(cur_df['label'])==1):
            tot_got+=1
            tot_val+= tot_got/(i+1)
            #if (tot_got==n):
                #print(i)
    
    return tot_val/n

ap1 = my_map(Test_data1, y_belong, np.array(preds[0]), 0)
ap2 = my_map(Test_data2, y_test_2, np.array(preds[1]), 1)


print("map = ", (ap1 + ap2 + ap3 + ap4 + ap5 + ap6 + ap7 + ap8)/8)