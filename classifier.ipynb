{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit39b187e956f148688106743f51c2bd62",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')# download nltk files\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchnlp.nn as nlp_nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "torch.manual_seed(1)\n",
    "# from tensorflow import summary\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TE(nn.Module):\n",
    "    def __init__(self, embedding_dim,hidden_dim, vocab_size, class_numbers):\n",
    "        super(LSTM_TE, self).__init__()\n",
    "        self.H = hidden_dim\n",
    "        self.K = class_numbers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.H)\n",
    "        self.att = nn.Linear(self.H, self.K,bias = True)\n",
    "        self.att_norm = nn.Softmax(dim = 0)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.fin_ = nn.Softmax(dim = 0)\n",
    "        self.fcs = [ nn.Linear(self.H,1) for _ in range(self.K)]\n",
    "        self.c_prob = [ nn.Sigmoid() for _ in range(self.K)]\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out = lstm_out.view(len(sentence),self.H)\n",
    "        att_denorm = self.att(lstm_out.view(len(sentence), -1)) # a_t = W_att * h_t + b_att\n",
    "        att_denorm = self.act(att_denorm)       # a_t = sigmoid(a_t)\n",
    "        As = self.att_norm(att_denorm) # A = softmax_row({a_1,..,a_d}^T)\n",
    "        As = As.view(len(sentence),self.K) # A = D*K\n",
    "        T = torch.zeros((self.H,self.K))\n",
    "        for i in range(self.H):\n",
    "          T[i] = torch.mul(torch.transpose(As,0,1),torch.transpose(lstm_out,0,1)[i]).sum(dim=1)\n",
    "        # tag_scores  = torch.mul(As,lstm_out) # produce T = {T1,..,TK} \n",
    "        # tag_scores = self.fin_(tag_scores.sum(dim = 0)) \n",
    "        T = torch.transpose(T,0,1)\n",
    "\n",
    "        rt = torch.zeros((self.K,1)).to(device)\n",
    "        for i in range(self.K):\n",
    "          rt[i] = self.fcs[i](T[i])\n",
    "          rt[i] = self.c_prob[i](rt[i])\n",
    "        return  rt\n",
    "    def init_hidden(self,batch_size):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(1, batch_size, self.hidden_dim).zero_().to(device),\n",
    "             weights.new(1, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stop(txt):\n",
    "  return [ c for c in txt if c not in stopwords.words(\"english\")]\n",
    "stemmer = PorterStemmer()\n",
    "def word_stemm(txt):\n",
    "  return [stemmer.stem(w) for w in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_files_train = glob.glob(\"./dataset/Task1/Train/*.xlsx\")\n",
    "xl_files_test = glob.glob(\"./dataset/Task1/Test/*.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['./dataset/Task1/Test/Loss.xlsx',\n './dataset/Task1/Test/Frustrative_Nonreward.xlsx',\n './dataset/Task1/Test/Sleep_Wakefulness.xlsx',\n './dataset/Task1/Test/Sustained_Threat.xlsx',\n './dataset/Task1/Test/Potential_Threat_Anxiety.xlsx',\n './dataset/Task1/Test/Circadian_Rhythms.xlsx',\n './dataset/Task1/Test/Arousal.xlsx',\n './dataset/Task1/Test/Acute_Threat_Fear.xlsx']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "xl_files_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "labels = {}\n",
    "revLabels = {}\n",
    "for i,xl in  enumerate(xl_files_train):\n",
    "    className = xl.split(\"/\")[-1][:-5]\n",
    "    labels[className] = i\n",
    "    revLabels[i] = className"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataframes\n",
    "frames = []\n",
    "for xl_file in xl_files_train:\n",
    "    df1 = pd.read_excel(xl_file)\n",
    "    className = xl_file.split(\"/\")[-1][:-5]\n",
    "    df1['Y'] = labels[className]\n",
    "    frames.append(df1)\n",
    "df_train = pd.concat(frames,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataframes\n",
    "frames = []\n",
    "for xl_file in xl_files_test:\n",
    "    df1 = pd.read_excel(xl_file)\n",
    "    className = xl_file.split(\"/\")[-1][:-5]\n",
    "    df1['Y'] = labels[className]\n",
    "    frames.append(df1)\n",
    "df_test = pd.concat(frames,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process the text\n",
    "stop_words = stopwords.words(\"english\")\n",
    "cols = ['title','abstract']\n",
    "for col in cols :\n",
    "    df_train['c_'+col] = df_train[col].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in stop_words]))\n",
    "    df_test['c_'+col] = df_test[col].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTensor(inp):\n",
    "  return torch.tensor(inp,dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the entire text\n",
    "df_train['inp'] =  df_train['c_title'] + df_train['c_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['inp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_trainx = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trainx = vectorizer.transform(x_train).toarray()\n",
    "x_trainx = x_trainx.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainx = list(map(toTensor,x_trainx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4277"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "len(x_trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vectorizer.vocabulary_.keys()) + 1\n",
    "CLASS_SIZE = 8\n",
    "EMBED_DIM = 1\n",
    "HIDDEN_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_train['Y'].apply(toTensor).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_TE(EMBED_DIM, HIDDEN_DIM, VOCAB_SIZE, CLASS_SIZE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "358.9650790691376\n354.49069690704346\n353.1128478050232\n352.3536539077759\n653.7088289260864\n"
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    st_time = time.time() \n",
    "    for i in range(len(x_trainx)):\n",
    "        model.zero_grad()\n",
    "        loss = loss_function( model( x_trainx[i] ).reshape((1,8)) , Y_train[i].reshape((1)) )\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print( time.time() - st_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns = ['pmid', 'title', 'abstract', 'Belongs','Relevant Context', 'Y', 'c_title', 'c_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['inp'] = df_test['c_title']  + df_test['c_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test['inp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = list(vectorizer.transform( x_test ).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., ..., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['inp2'] = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = df_test.groupby('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap(y_pred,y_belong):\n",
    "  y_pred = np.array(y_pred)\n",
    "  y_belong = np.array(y_belong)\n",
    "  y_pred = y_pred.reshape(-1,1)\n",
    "  y_belong = y_belong.reshape(-1,1)\n",
    "  arr = np.concatenate((y_pred,y_belong),axis=1)\n",
    "  \n",
    "  arr = sorted(arr,key = lambda x:x[0],reverse=True)\n",
    "  \n",
    "  ans = 0\n",
    "  rel = 0\n",
    "  num = 0\n",
    "  for i in range(len(y_pred)):\n",
    "    if arr[i][1] == 1:\n",
    "      rel = rel+1\n",
    "      ans += rel/(i+1)\n",
    "      num = num+1\n",
    "  return ans/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.groups\n",
    "construct_test = {}\n",
    "for i in range(8):\n",
    "  construct_test[i]= df_test.iloc[gdf.groups[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loss 0.4675430554538911\nFrustrative_Nonreward 0.5317705800044853\nSleep_Wakefulness 0.9727107757178387\nSustained_Threat 0.409829556172487\nPotential_Threat_Anxiety 0.8328834727883865\nCircadian_Rhythms 1.0\nArousal 0.8378011540683261\nAcute_Threat_Fear 0.5770066252372037\n"
    }
   ],
   "source": [
    "map_list = []\n",
    "class_count = {}\n",
    "for _ in range(1):\n",
    "  aps = 0\n",
    "  for cl_ in construct_test.keys():\n",
    "    X_test = construct_test[cl_]['inp2'].apply(toTensor).values\n",
    "    y_pred = torch.randn((len(X_test),1)).to(device)\n",
    "    class_count[cl_] = len(X_test)\n",
    "    for i in range(len(X_test)):\n",
    "      y_pred[i] = model(X_test[i].to(device))[cl_,0].reshape(1,1)\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    # print(y_pred.shape)\n",
    "    y_belongs = np.array(construct_test[cl_]['Belongs'].values).reshape(len(X_test),1)\n",
    "    # print(y_belongs.shape)\n",
    "    # print(np.concatenate((y_belongs,y_pred),axis =1 ))\n",
    "    t = ap(y_pred,y_belongs)\n",
    "    print(revLabels[cl_],t)\n",
    "    aps += t\n",
    "  # println()\n",
    "  map_list.append(aps/8)\n",
    "  # break\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-c3dd0532462e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# print(np.concatenate((y_belongs,y_pred),axis =1 ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_belongs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# println()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "map_list = []\n",
    "class_count = {}\n",
    "for _ in range(1):\n",
    "  aps = 0\n",
    "  for cl_ in construct_test.keys():\n",
    "    X_test = construct_test[cl_]['inp2'].apply(toTensor).values\n",
    "    y_pred = torch.randn((len(X_test),1)).to(device)\n",
    "    class_count[cl_] = len(X_test)\n",
    "    for i in range(len(X_test)):\n",
    "      y_pred[i] = model(X_test[i].to(device))[cl_,0].reshape(1,1)\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    # print(y_pred.shape)\n",
    "    y_belongs = np.array(construct_test[cl_]['Belongs'].values).reshape(len(X_test),1)\n",
    "    # print(y_belongs.shape)\n",
    "    # print(np.concatenate((y_belongs,y_pred),axis =1 ))\n",
    "    t = ap(y_pred,y_belongs)\n",
    "    print(labels[cl_],t)\n",
    "    aps += t\n",
    "  # println()\n",
    "  map_list.append(aps/8)\n",
    "  # break\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap(y_pred,y_belong):\n",
    "  y_pred = np.array(y_pred)\n",
    "  y_belong = np.array(y_belong)\n",
    "  y_pred = y_pred.reshape(-1,1)\n",
    "  y_belong = y_belong.reshape(-1,1)\n",
    "  arr = np.concatenate((y_pred,y_belong),axis=1)\n",
    "  \n",
    "  arr = sorted(arr,key = lambda x:x[0],reverse=True)\n",
    "  \n",
    "  ans = 0\n",
    "  rel = 0\n",
    "  num = 0\n",
    "  for i in range(len(y_pred)):\n",
    "    if arr[i][1] == 1:\n",
    "      rel = rel+1\n",
    "      ans += rel/(i+1)\n",
    "      num = num+1\n",
    "  return ans/num\n",
    "\n",
    "def map(y_pred,y,y_belong):\n",
    "  j = 0\n",
    "  j_prev = 0\n",
    "  ans = 0\n",
    "  for i in range(8):\n",
    "    while j<len(y) and y[j] == i:\n",
    "      j = j+1\n",
    "    ans = ans + ap(y_pred[j_prev:j],y_belong[j_prev:j])\n",
    "    j_prev = j\n",
    "  return ans/8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}